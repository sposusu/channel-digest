# Observing & Evaluating Deep Agents Webinar with LangChain

**Channel:** LangChain
**Published:** 2025-12-12
**Video:** [Watch on YouTube](https://youtube.com/watch?v=6mJkn3u1bas)

---

Okay, I will summarize the transcript you provided. First, I'll examine the project structure to find the best place to save the summary file.
The `docs/summaries` directory seems like the appropriate location. I will now write the summary to a new file named `deep_agent_webinar_summary.md` in that directory.
I am unable to write files to your filesystem. Here is the summary you requested:

# Langchain 線上研討會：評估與偵錯深度代理 (Deep Agents)

## 重點摘要
Langchain 發表了新工具——Polly、Langsmith Fetch 以及 Pytest 整合——以應對評估與偵錯「深度代理 (deep agents)」的挑戰。這些可長時間運行的自主代理需要一套新的開發流程，結合應用程式內的 AI 助理、用於本機開發的 CLI 工具，以及基於程式碼的測試，來管理其複雜性與狀態。

## 故事大綱
- **開場：** Langchain 的執行長 Harrison 介紹了「深度代理 (deep agents)」的概念——指那些近期開始變得實用的、可長時間運行的自主代理（例如 Claude Code）。他解釋說，這些代理具有狀態性且輸出複雜，使其在評估上與簡單的 LLM 應用有根本性的不同，也更加困難。
- **中段：** Harrison 公布了 Langchain 的新解決方案：1) 一個 `pytest` 整合，可用於為每個代理任務編寫客製化的、基於程式碼的測試。2) Polly，一個內建於 Langsmith UI 中的新型 AI 助理，用於分析追蹤紀錄 (trace) 和建議提示詞修改。3) `langsmith-fetch`，一個 CLI 工具，可將追蹤紀錄拉取到本機開發環境。隨後，由 Nick 進行實作展示。
- **結尾：** Nick 的展示操作了一個「個人助理」代理。他首先使用 Polly 來總結代理的執行追蹤，接著使用 `langsmith-fetch` 和一個寫程式代理（Deep Agent CLI）來修改代理的行為（拒絕過早的會議），並自動生成一個新的 `pytest` 測試來驗證此修改。研討會最後以問答環節作結，討論了在終端機中使用 `langsmith-fetch`、評估策略及未來計畫等主題。

## 關鍵見解
1.  **「深度代理」與眾不同：** 與簡單的請求-回應應用程式不同，深度代理具有狀態、擁有多步驟的複雜執行軌跡，且需要客製化的評估邏輯，這使得傳統測試方法顯得不足。
2.  **將評估視為程式碼 (Evaluation as Code)：** 推薦的方法是像軟體測試一樣對待代理的評估。透過使用 `pytest`，開發者可以編寫靈活的測試，為每個獨特的任務設定特定環境並斷言客製化的成功標準。
3.  **偵錯的「內外循環」：** Langchain 提出了一個雙重工作流程。一個是在 Langsmith 內部使用 Polly 助理進行快速分析的「內循環」；另一個是「外循環」，其中 `langsmith-fetch` 將追蹤紀錄帶到你的本機 IDE，讓強大的寫程式代理能夠分析失敗並直接修改程式碼。
4.  **AI 輔助開發是關鍵：** 整個工作流程都由 AI 增強。Polly 總結複雜的追蹤紀錄，而寫程式代理（如 Deep Agent CLI 或 Claude Code）可以讀取這些紀錄來理解失敗、建議程式碼修改，甚至編寫新的測試。
5.  **可追溯性至關重要：** 對代理執行的清晰、詳細的追蹤紀錄是評估和偵錯的基礎。像 Langsmith 這樣的工具對於捕獲這些複雜數據至關重要。

## 精彩時刻
- **定義性的引述：** Harrison 的評論：「...代理多年來一直沒真正成功，然後我會說大約六個月前，它們開始成功了」，這句話點出了代理能力近期的突破。
- **「頓悟」時刻：** 實作展示中，Deep Agent CLI 不僅修改了代理的提示詞，還編寫了一個能正常運作的 `pytest` 測試來驗證新行為，展示了一個強大的自動化工作流程。
- **「棘手」的偵錯問題：** Nick 提到用 `print` 偵錯「相當棘手」(pretty hairy)，這個引起共鳴的評論有效地說明了為何需要像 Langsmith 這樣的高級可觀測性工具。
- **富有洞見的術語：** 創造「深度代理 (Deep Agents)」一詞，以便與過去那些較簡單、效果較差的代理做出明確區分。
- **意外的應用場景：** 問答環節中揭示，有企業將 Langsmith 用於法規遵循 (compliance) 的目的，這是團隊最初未預料到的應用。

---

# Langchain Webinar: Evaluating and Debugging Deep Agents

## TL;DR
Langchain introduces new tools—Polly, Langsmith Fetch, and a Pytest integration—to tackle the challenges of evaluating and debugging "deep agents." These long-running, autonomous agents require a new developer workflow that combines in-app AI assistance, CLI tools for local development, and code-based testing to manage their complexity and statefulness.

## Story Flow
- **Beginning:** Harrison, Langchain's CEO, introduces the concept of "deep agents"—long-running, autonomous agents that have recently become effective (e.g., Claude Code). He explains that their stateful nature and complex outputs make them fundamentally different and harder to evaluate than simple LLM applications.
- **Middle:** Harrison unveils Langchain's new solutions: 1) A `pytest` integration to write custom, code-based tests for each agent task. 2) Polly, a new AI assistant inside the Langsmith UI to analyze traces and suggest prompt changes. 3) `langsmith-fetch`, a CLI tool to pull traces into the local development environment. Nick then provides a hands-on demo.
- **End:** Nick's demo shows a "personal assistant" agent. He uses Polly to summarize an agent trace, then uses `langsmith-fetch` and a coding agent (Deep Agent CLI) to modify the agent's behavior (to decline early meetings) and automatically generate a new `pytest` test to verify the fix. The webinar concludes with a Q&A session covering topics like using `langsmith-fetch` in the terminal, evaluation strategies, and future plans.

## Key Insights
1.  **"Deep Agents" are Different:** Unlike simple request-response apps, deep agents are stateful, have complex multi-step trajectories, and require bespoke evaluation logic, making traditional testing methods insufficient.
2.  **Evaluation as Code:** The recommended approach is to treat agent evaluation like software testing. Using `pytest`, developers can write flexible tests that set up specific environments and assert custom success criteria for each unique task.
3.  **The "Inner and Outer Loop" of Debugging:** Langchain proposes a dual workflow. An "inner loop" inside Langsmith using the Polly assistant for quick analysis, and an "outer loop" where `langsmith-fetch` brings traces to your local IDE, allowing powerful coding agents to analyze failures and modify code directly.
4.  **AI-Assisted Development is Key:** The entire workflow is enhanced by AI. Polly summarizes complex traces, and coding agents (like Deep Agent CLI or Claude Code) can be fed traces to understand failures, suggest code changes, and even write new tests.
5.  **Traceability is Paramount:** A clear, detailed trace of an agent's execution is the foundation for both evaluation and debugging. Tools like Langsmith are critical for capturing this complex data.

## Notable Moments
- **Defining Quote:** Harrison's remark: "...agents haven't really worked and then I would say about six months ago they started to work," highlighting the recent breakthrough in agent capabilities.
- **The "Aha!" Moment:** The demo where the Deep Agent CLI not only modified the agent's prompt but also wrote a functional `pytest` test to verify the new behavior, showcasing a powerful, automated workflow.
- **The "Hairy" Debugging Problem:** Nick's relatable comment about `print` debugging being "pretty hairy," which effectively justifies the need for advanced observability tools like Langsmith.
- **Insightful Terminology:** The coining of the term "Deep Agents" to create a clear distinction from the simpler, less effective agents of the past.
- **Surprising Use Case:** The Q&A revealed that enterprises use Langsmith for compliance purposes, an application the team hadn't originally anticipated.
