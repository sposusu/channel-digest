# Product Evals (for AI Applications) in Three Simple Steps

**Channel:** LangChain
**Published:** 2025-12-01
**Video:** [Watch on YouTube](https://youtube.com/watch?v=mz7mAo4zIC8)

---

# 如何使用 Langsmith 建立 LLM 產品評估 (遵循 Eugene Yan 的三步驟流程)

## 重點摘要
本影片示範如何使用 Langsmith 工具，實踐知名 AI 專家 Eugene Yan 提出的三步驟評估（Eval）流程，以建立客製化且可靠的 LLM 應用評估機制。核心概念是透過「標註資料、校準評估器、運行測試」的迭代循環，持續優化你的 LLM 應用，確保其表現符合預期。

## 故事大綱
- **開場**：影片開頭引用了 Eugene Yan 的一則推文，點出建立產品評估的三個關鍵步驟：標註小型資料集、校準 LLM 評估器、以及在每次修改後運行評估。講者宣告將在 Langsmith 中完整演示這套流程。

- **中段**：進入 Langsmith 平台進行實作演練。
    1.  **建立專案與資料集**：設定一個將部落格文章轉為推文的簡單應用，並從 LangChain 官網複製幾篇文章作為測試資料。
    2.  **第一步：標註資料**：講者定義了一個非常主觀但關鍵的評估標準：「聽起來是否像 AI 生成的？」(例如：語氣太過油膩、行銷味重、或使用 M-dash 破折號)。他手動為幾個初步生成的推文打上「像 AI」或「不像 AI」的標籤，並寫下判斷理由。
    3.  **第二步：校準 LLM 評估器**：利用剛標註好的資料，建立一個「以 LLM 為基礎的評估器」。初版評估器的判斷與人工標註的符合率很低，於是講者修改提示語，加入更明確的規則（如：「如果內文使用 M-dash 就是像 AI」、「聽起來太俗氣也是像 AI」），直到評估器的判斷與人工標註 100% 一致。
    4.  **第三步：運行評估**：將這個校準好的評估器應用到原始專案上。初次運行結果顯示所有推文都「像 AI」。講者接著修改應用的主提示語（加入「不要聽起來像 AI」、「別用 M-dash」等指令），再次運行後，成功看到「像 AI」的比例下降，證明了應用的改進。

- **結尾**：講者強調這是一個持續迭代的過程。他將新產出的、品質更好的推文再次進行人工標註（標為「聽起來不錯」），並用來進一步校準評估器，使其更精準。影片總結，Langsmith 的價值不只是追蹤評估結果，更是提供一套完整的工具，幫助開發者輕鬆建立、校準並運行專屬於自己產品的評估流程。

## 關鍵見解
1.  **客製化評估至關重要**：通用的現成評估標準遠遠不夠。有效的評估應針對產品的特定目標，例如「聽起來要真誠、有技術感」或「不能太俗氣」。
2.  **「標註、校準、運行」的黃金循環**：建立 LLM 評估是一個迭代過程。首先手動標註少量資料，接著校準一個 LLM 評估器來模仿你的判斷，最後在每次修改後都用它來衡量進步。
3.  **讓 LLM 成為你的評估助理**：你可以訓練一個 LLM 來自動化評估工作，但前提是必須先用少量人工標註的範例來「校準」它，使其判斷標準與你對齊。
4.  **從一小步開始，持續迭代**：不需要一開始就準備龐大的資料集。從幾個標註範例開始，建立你的第一個評估器，然後在後續的開發中不斷加入新的標註資料來完善它。
5.  **Langsmith 的核心價值**：Langsmith 不僅僅是個結果追蹤器，它是一個協助你完成「建立、校準、運行」整個客製化評估流程的工作平台。

## 精彩時刻
- **M-dash 規則**：一個非常具體的判斷標準：「如果推文主體包含 M-dash（—），那它聽起來就像 AI。」這個例子將抽象的「AI 感」變得非常明確。
- **「太俗氣」或「行銷味太重」**：講者使用如「`supercharges` 這個詞聽起來太像 AI」或「太俗氣」等主觀標準，完美詮釋了為何評估標準需要針對產品的獨特風格進行客製。
- **迭代循環的實際展演**：當他將改善後的推文標註為「好範例」，並反過來用它去優化評估器本身時，清楚地展示了這個流程的循環與自我增強特性。
- **經典引言**：「Langsmith 不僅僅是為了追蹤你的評估結果，它讓建立這些評估本身變得更容易。它不試圖給你現成的評估，而是引導你走過這三個獨立的評估步驟……」

---

# How to Build LLM Product Evals with Langsmith (Following Eugene Yan's 3-Step Process)

## TL;DR
This video demonstrates how to use Langsmith to implement Eugene Yan's three-step process for building robust LLM evaluations. The core concept involves creating a custom, labeled dataset, aligning an LLM to act as a judge based on those labels, and then iteratively running this evaluation harness to refine your application's prompts and configuration.

## Story Flow
- **Beginning:** The video starts by referencing a tweet from Eugene Yan that outlines three key steps for building product evals: labeling a small dataset, aligning an LLM evaluator, and running the eval harness with each change. The speaker announces he will demonstrate this entire workflow within Langsmith.

- **Middle:** The presentation moves into a hands-on demo in the Langsmith platform.
    1.  **Project & Dataset Setup:** A simple application is created to turn blog posts into tweets, and a few articles from the LangChain blog are used to build the initial dataset.
    2.  **Step 1: Labeling Data:** The speaker defines a subjective yet crucial evaluation criterion: "Does it sound like AI?" (e.g., too cheesy, marketing-heavy, or uses m-dashes). He manually labels a few initial outputs as "sounds like AI" or not, providing his reasoning.
    3.  **Step 2: Aligning the LLM Evaluator:** Using the newly labeled data, an "LM-as-a-judge" is created. The initial version has low alignment with the human labels, so the speaker refines the evaluator's prompt with more specific rules (e.g., "if it includes m-dashes in the main body, that sounds like AI," "if it sounds too corny, that's also AI-like") until it achieves 100% alignment.
    4.  **Step 3: Running the Eval Harness:** This calibrated evaluator is then applied to the original project. The first run shows that all the tweets "sound like AI." The speaker then modifies the main application's prompt (adding instructions like "Don't sound like AI. Don't use m-dashes."), runs the evaluation again, and successfully sees the "sounds like AI" score decrease, proving the application's improvement.

- **End:** The speaker emphasizes that this is an iterative process. He takes the new, higher-quality outputs, labels them again (this time as "sounds good"), and uses them to further refine the evaluator, making it even more accurate. The video concludes that Langsmith's value lies not just in tracking results, but in providing a complete toolkit to easily build, align, and run evaluations tailored to your specific product.

## Key Insights
1.  **Custom Evals are Crucial:** Off-the-shelf evaluations are insufficient. Evals should be tailored to specific product goals, such as "doesn't sound too corny" or "is authentic and technical."
2.  **The "Label, Align, Run" Loop:** Effective LLM evaluation is an iterative process: 1) Manually label a small dataset with binary pass/fail criteria. 2) Align an LLM-as-a-judge to replicate your labeling. 3) Run this evaluator against every application change to measure improvement.
3.  **LLM-as-a-Judge:** You can use an LLM to automate evaluations, but it must first be "aligned" with human judgment by training it on a small set of manually labeled examples.
4.  **Start Small and Iterate:** You don't need a massive dataset to begin. Start with a small, labeled set, build your first evaluator, and continuously refine it by adding more labeled examples from subsequent runs.
5.  **Langsmith's Role:** Langsmith is presented not just as a tracker for results, but as a workbench that facilitates the entire workflow of building, aligning, and running these custom evaluators.

## Notable Moments
- **The "M-dash" Rule:** A specific, concrete example of a rule for the evaluator: "if it includes m-dashes in the main body, that sounds like AI." This makes the abstract concept of "AI-like" very tangible.
- **"Too Corny" or "Marketing-like":** The speaker's subjective but important criteria, like "`supercharges` sounds like AI" or "too corny," highlight the need for custom, product-specific evaluation metrics.
- **The Iterative Loop in Action:** The moment he takes the *improved* outputs, labels them as "good," and then uses them to *further refine* the evaluator, clearly demonstrating the cyclical and self-improving nature of the process.
- **Memorable Quote:** "Langsmith isn't just for tracking your eval results. It also makes it easier to build these evals. It doesn't try to give you off-the-shelf evals, but rather walks you through these three separate eval steps..."
